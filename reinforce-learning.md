
### Learning to learn by gradient descent by gradient descent
#### abstract
--how the design of an optimization algorithm can be cast as a learning problem,  
--allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way  
  
#### introduction
deep learning optimization methods:  
momentum/Rprop/Adagrad/Adadelta/RMSprop/ADAM  
  
  according to the _No Free Lunch Theorems for Optimization_, in the setting of combinatorial optimization,
no algorithm is able to do better than a random strategy in expectation  
specialization to a subclass of problems is in fact
the only way that improved performance can be achieved in general  
